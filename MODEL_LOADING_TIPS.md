# 模型加载说明

## 为什么模型加载会"卡住"？

模型加载时显示 "Loading checkpoint shards: 0%|" 并长时间不动是**正常现象**，原因如下：

### 1. 模型文件很大
- Qwen2.5-7B-Instruct 模型文件通常有 10-20GB
- 即使使用4bit量化，也需要加载多个检查点文件
- 从磁盘读取大文件需要时间

### 2. 加载过程
模型加载包括以下步骤（每个步骤都需要时间）：
1. 读取模型配置文件
2. 加载检查点分片（checkpoint shards）- **这一步最慢**
3. 将权重加载到GPU内存
4. 应用量化（如果使用4bit）
5. 加载LoRA适配器

### 3. 正常加载时间
- **首次加载**：5-15分钟（取决于硬件）
- **后续加载**：2-5分钟（如果模型已缓存）

## 如何判断是否正常？

### ✅ 正常情况
- 看到 "Loading checkpoint shards" 进度条（即使很慢）
- CPU/GPU使用率较高
- 内存使用逐渐增加
- 没有错误信息

### ⚠️ 可能有问题
- 完全卡住超过30分钟
- CPU/GPU使用率为0
- 出现错误信息
- 内存使用不增加

## 优化加载速度

### 1. 使用本地模型路径
如果模型已下载到本地，使用本地路径：
```python
model = CourtDebateModel(
    adapter_dir="court_debate_model",
    base_model="D:/huggingface_models/unsloth/Qwen2.5-7B-Instruct",  # 本地路径
    load_in_4bit=True,
    gpu_id=0
)
```

### 2. 确保使用SSD
- 如果模型在机械硬盘上，加载会很慢
- 建议将模型放在SSD上

### 3. 检查磁盘空间
- 确保有足够的磁盘空间（至少20GB）

### 4. 使用更快的网络（如果从网络下载）
- 如果从HuggingFace下载，使用镜像站点：
```bash
set HF_ENDPOINT=https://hf-mirror.com
```

## 常见问题

### Q: 加载了10分钟还没完成，正常吗？
A: 正常。大模型首次加载通常需要10-20分钟。

### Q: 如何查看加载进度？
A: 运行诊断脚本会显示加载时间：
```bash
python diagnose_speed.py
```

### Q: 可以中断加载吗？
A: 可以按 Ctrl+C 中断，但下次需要重新加载。

### Q: 加载后模型会保存在内存中吗？
A: 是的，模型会一直占用GPU内存，直到程序结束。

### Q: 如何加快后续加载？
A: 
1. 使用本地模型路径（避免网络下载）
2. 使用SSD存储模型
3. 考虑使用更小的模型

## 性能参考

### RTX 4060 Laptop GPU (8GB)
- **首次加载**：约10-15分钟
- **后续加载**：约3-5分钟
- **4bit量化后模型大小**：约4-5GB

### 更快的GPU（RTX 4090等）
- **首次加载**：约5-10分钟
- **后续加载**：约2-3分钟

## 如果加载失败

1. **检查错误信息**
   - 查看完整的错误输出
   - 检查是否是内存不足

2. **检查磁盘空间**
   ```bash
   # Windows
   dir D:\huggingface_models
   ```

3. **检查GPU内存**
   ```python
   import torch
   print(f"GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
   ```

4. **尝试减少量化**
   - 如果4bit加载失败，尝试不使用量化（需要更多内存）

5. **检查模型路径**
   - 确保模型路径正确
   - 检查文件是否完整

## 总结

**模型加载慢是正常的**，特别是首次加载。请耐心等待，通常10-20分钟内会完成。如果超过30分钟还没有进展，再考虑检查是否有问题。

